{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9d22d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38220c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt').read().splitlines()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6ef9ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('.', '.', 'e'): 2, ('.', 'e', 'm'): 1, ('e', 'm', 'm'): 1, ('m', 'm', 'a'): 1, ('m', 'a', '.'): 1, ('.', '.', 'o'): 1, ('.', 'o', 'l'): 1, ('o', 'l', 'i'): 1, ('l', 'i', 'v'): 1, ('i', 'v', 'i'): 1, ('v', 'i', 'a'): 1, ('i', 'a', '.'): 4, ('.', '.', 'a'): 2, ('.', 'a', 'v'): 1, ('a', 'v', 'a'): 1, ('v', 'a', '.'): 1, ('.', '.', 'i'): 1, ('.', 'i', 's'): 1, ('i', 's', 'a'): 1, ('s', 'a', 'b'): 1, ('a', 'b', 'e'): 1, ('b', 'e', 'l'): 1, ('e', 'l', 'l'): 1, ('l', 'l', 'a'): 1, ('l', 'a', '.'): 1, ('.', '.', 's'): 1, ('.', 's', 'o'): 1, ('s', 'o', 'p'): 1, ('o', 'p', 'h'): 1, ('p', 'h', 'i'): 1, ('h', 'i', 'a'): 1, ('.', '.', 'c'): 1, ('.', 'c', 'h'): 1, ('c', 'h', 'a'): 1, ('h', 'a', 'r'): 2, ('a', 'r', 'l'): 1, ('r', 'l', 'o'): 1, ('l', 'o', 't'): 1, ('o', 't', 't'): 1, ('t', 't', 'e'): 1, ('t', 'e', '.'): 1, ('.', '.', 'm'): 1, ('.', 'm', 'i'): 1, ('m', 'i', 'a'): 1, ('.', 'a', 'm'): 1, ('a', 'm', 'e'): 1, ('m', 'e', 'l'): 1, ('e', 'l', 'i'): 1, ('l', 'i', 'a'): 1, ('.', '.', 'h'): 1, ('.', 'h', 'a'): 1, ('a', 'r', 'p'): 1, ('r', 'p', 'e'): 1, ('p', 'e', 'r'): 1, ('e', 'r', '.'): 1, ('.', 'e', 'v'): 1, ('e', 'v', 'e'): 1, ('v', 'e', 'l'): 1, ('e', 'l', 'y'): 1, ('l', 'y', 'n'): 1, ('y', 'n', '.'): 1}\n"
     ]
    }
   ],
   "source": [
    "t = {}\n",
    "for w in words[:10]:\n",
    "    chs = ['.', '.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        t[(ch1,ch2,ch3)] = t.get((ch1,ch2,ch3), 0) + 1\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "264a1b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('i', 'a', '.'), 4),\n",
       " (('.', '.', 'e'), 2),\n",
       " (('.', '.', 'a'), 2),\n",
       " (('h', 'a', 'r'), 2),\n",
       " (('.', 'e', 'm'), 1),\n",
       " (('e', 'm', 'm'), 1),\n",
       " (('m', 'm', 'a'), 1),\n",
       " (('m', 'a', '.'), 1),\n",
       " (('.', '.', 'o'), 1),\n",
       " (('.', 'o', 'l'), 1),\n",
       " (('o', 'l', 'i'), 1),\n",
       " (('l', 'i', 'v'), 1),\n",
       " (('i', 'v', 'i'), 1),\n",
       " (('v', 'i', 'a'), 1),\n",
       " (('.', 'a', 'v'), 1),\n",
       " (('a', 'v', 'a'), 1),\n",
       " (('v', 'a', '.'), 1),\n",
       " (('.', '.', 'i'), 1),\n",
       " (('.', 'i', 's'), 1),\n",
       " (('i', 's', 'a'), 1),\n",
       " (('s', 'a', 'b'), 1),\n",
       " (('a', 'b', 'e'), 1),\n",
       " (('b', 'e', 'l'), 1),\n",
       " (('e', 'l', 'l'), 1),\n",
       " (('l', 'l', 'a'), 1),\n",
       " (('l', 'a', '.'), 1),\n",
       " (('.', '.', 's'), 1),\n",
       " (('.', 's', 'o'), 1),\n",
       " (('s', 'o', 'p'), 1),\n",
       " (('o', 'p', 'h'), 1),\n",
       " (('p', 'h', 'i'), 1),\n",
       " (('h', 'i', 'a'), 1),\n",
       " (('.', '.', 'c'), 1),\n",
       " (('.', 'c', 'h'), 1),\n",
       " (('c', 'h', 'a'), 1),\n",
       " (('a', 'r', 'l'), 1),\n",
       " (('r', 'l', 'o'), 1),\n",
       " (('l', 'o', 't'), 1),\n",
       " (('o', 't', 't'), 1),\n",
       " (('t', 't', 'e'), 1),\n",
       " (('t', 'e', '.'), 1),\n",
       " (('.', '.', 'm'), 1),\n",
       " (('.', 'm', 'i'), 1),\n",
       " (('m', 'i', 'a'), 1),\n",
       " (('.', 'a', 'm'), 1),\n",
       " (('a', 'm', 'e'), 1),\n",
       " (('m', 'e', 'l'), 1),\n",
       " (('e', 'l', 'i'), 1),\n",
       " (('l', 'i', 'a'), 1),\n",
       " (('.', '.', 'h'), 1),\n",
       " (('.', 'h', 'a'), 1),\n",
       " (('a', 'r', 'p'), 1),\n",
       " (('r', 'p', 'e'), 1),\n",
       " (('p', 'e', 'r'), 1),\n",
       " (('e', 'r', '.'), 1),\n",
       " (('.', 'e', 'v'), 1),\n",
       " (('e', 'v', 'e'), 1),\n",
       " (('v', 'e', 'l'), 1),\n",
       " (('e', 'l', 'y'), 1),\n",
       " (('l', 'y', 'n'), 1),\n",
       " (('y', 'n', '.'), 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(t.items(), key = lambda kv : -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35fa333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency lookup : N - How many times a character occurs given the previous two characters \n",
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "N = torch.ones([27, 27, 27], dtype=torch.int32) # smoothing\n",
    "stoi = {c:i for i, c in enumerate(chars)}\n",
    "itos = {v:k for k, v in stoi.items()}\n",
    "for w in words:\n",
    "    chs = ['.', '.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        idx3 = stoi[ch3]\n",
    "        N[idx1,idx2, idx3] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8da06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual seed for reproducibility\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f57b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce.\n",
      "bra.\n",
      "jalius.\n",
      "rochityharlonimittain.\n",
      "luwak.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    idx1, idx2 = 0, 0\n",
    "    out=''\n",
    "    while True:\n",
    "        # extract count\n",
    "        freq = N[idx1, idx2].float()\n",
    "\n",
    "        # get probabailities\n",
    "        probs = freq / freq.sum()\n",
    "        \n",
    "        # shift the context forward\n",
    "        idx1 = idx2 \n",
    "\n",
    "        # sample\n",
    "        idx2 = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        out += itos[idx2]\n",
    "\n",
    "        if idx2==0:\n",
    "            break\n",
    "\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc4b2014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probs once instead of calculating everytime\n",
    "P = N.float()\n",
    "P /= P.sum(2, keepdims=True) # For each context (i, j), choose the next character i.e. \n",
    "# For fixed (i, j), sum over k equals 1. axis 0,1,2 is N[i,j,k]\n",
    "# i,j are prev chars and k is the next char given i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633766e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce.\n",
      "bra.\n",
      "jalius.\n",
      "rochityharlonimittain.\n",
      "luwak.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # must be in the same cell as it should be ran every time to generate same o/p\n",
    "\n",
    "for i in range(5):\n",
    "    idx1, idx2 = 0, 0\n",
    "    out=''\n",
    "    while True:\n",
    "        probs = P[idx1, idx2]\n",
    "        \n",
    "        # shift the context forward\n",
    "        idx1 = idx2 \n",
    "\n",
    "        # sample\n",
    "        idx2 = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "\n",
    "        out += itos[idx2]\n",
    "\n",
    "        if idx2==0:\n",
    "            break\n",
    "\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e88f324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loglikelihood: -504653.0\n",
      "negative loglikelihood: 504653.0\n",
      "avg negative loglikelihood: 2.2119739055633545\n"
     ]
    }
   ],
   "source": [
    "# loss calculation\n",
    "log_likelihood = 0\n",
    "n=0\n",
    "for w in words:\n",
    "    chs = ['.','.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:],chs[2:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        idx3 = stoi[ch3]\n",
    "        \n",
    "        prob = P[idx1, idx2, idx3]\n",
    "        log_prob = torch.log(prob)\n",
    "        log_likelihood += log_prob\n",
    "        n+=1\n",
    "\n",
    "nll = - log_likelihood\n",
    "print(f'loglikelihood: {log_likelihood}')\n",
    "print(f'negative loglikelihood: {nll}')\n",
    "print((f'avg negative loglikelihood: {nll/n}'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5a39869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 2]) torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "# create a dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.', '.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        idx3 = stoi[ch3]\n",
    "        xs.append([idx1, idx2])\n",
    "        ys.append(idx3)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(xs.shape, ys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f19128bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37f9f9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 2, 27])\n"
     ]
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "print(xenc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c9ebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c617a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 54])\n"
     ]
    }
   ],
   "source": [
    "# concat or flatten to have 1 vector per sample for NN\n",
    "xenc = xenc.view(len(xs), 54)\n",
    "print(xenc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7cc507a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "190c376b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4812,  1.2482,  0.3723,  ...,  0.3125,  2.3701,  3.1549],\n",
       "        [ 0.5352,  0.2418, -0.2616,  ..., -1.6675,  0.6432,  1.0764],\n",
       "        [ 0.8098,  1.5744,  0.0988,  ..., -0.9052, -0.2371,  3.2653],\n",
       "        ...,\n",
       "        [ 0.3543,  0.8678,  0.0411,  ..., -1.8267,  1.9362, -0.1628],\n",
       "        [ 0.4902, -1.4578,  0.0746,  ..., -0.0480,  0.8478, -1.1147],\n",
       "        [ 1.5952, -0.6920, -0.7784,  ..., -1.5724,  0.8434, -0.9350]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54,27), requires_grad=True, generator=g)\n",
    "logits = xenc @ W\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59dcaee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0820, 0.0239, 0.0099,  ..., 0.0094, 0.0734, 0.1608],\n",
       "        [0.0237, 0.0177, 0.0107,  ..., 0.0026, 0.0264, 0.0407],\n",
       "        [0.0224, 0.0482, 0.0110,  ..., 0.0040, 0.0079, 0.2616],\n",
       "        ...,\n",
       "        [0.0230, 0.0385, 0.0168,  ..., 0.0026, 0.1119, 0.0137],\n",
       "        [0.0108, 0.0015, 0.0071,  ..., 0.0063, 0.0154, 0.0022],\n",
       "        [0.1024, 0.0104, 0.0095,  ..., 0.0043, 0.0483, 0.0082]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts  = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a590cd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2447, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-probs.log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d771029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 2]) torch.Size([228146])\n",
      "torch.Size([228146, 2, 27])\n",
      "after concat: torch.Size([228146, 54])\n"
     ]
    }
   ],
   "source": [
    "# complete version\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.','.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:],chs[2:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        idx3 = stoi[ch3]\n",
    "        xs.append([idx1,idx2])\n",
    "        ys.append(idx3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(xs.shape, ys.shape)\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "print(xenc.shape)\n",
    "\n",
    "xenc = xenc.view(len(xs),54)\n",
    "print('after concat:', xenc.shape)\n",
    "\n",
    "\n",
    "#initialization\n",
    "g = torch.Generator().manual_seed(456)\n",
    "W = torch.randn((54,27), requires_grad=True, generator=g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7163fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.44396710395813\n",
      "loss: 2.3847761154174805\n",
      "loss: 2.408766508102417\n",
      "loss: 2.3935599327087402\n",
      "loss: 2.443021059036255\n",
      "loss: 2.383620500564575\n",
      "loss: 2.4075112342834473\n",
      "loss: 2.3925271034240723\n",
      "loss: 2.4421226978302\n",
      "loss: 2.3825576305389404\n",
      "loss: 2.4063711166381836\n",
      "loss: 2.3915653228759766\n",
      "loss: 2.4412691593170166\n",
      "loss: 2.3815758228302\n",
      "loss: 2.4053313732147217\n",
      "loss: 2.3906681537628174\n",
      "loss: 2.440459966659546\n",
      "loss: 2.380666494369507\n",
      "loss: 2.4043774604797363\n",
      "loss: 2.389829158782959\n",
      "loss: 2.4396934509277344\n",
      "loss: 2.37982177734375\n",
      "loss: 2.403498649597168\n",
      "loss: 2.3890433311462402\n",
      "loss: 2.438967227935791\n",
      "loss: 2.3790345191955566\n",
      "loss: 2.402686595916748\n",
      "loss: 2.3883070945739746\n",
      "loss: 2.438279628753662\n",
      "loss: 2.3782992362976074\n",
      "loss: 2.4019320011138916\n",
      "loss: 2.3876144886016846\n",
      "loss: 2.4376280307769775\n",
      "loss: 2.377610683441162\n",
      "loss: 2.4012296199798584\n",
      "loss: 2.386962890625\n",
      "loss: 2.437009811401367\n",
      "loss: 2.376964569091797\n",
      "loss: 2.40057373046875\n",
      "loss: 2.3863494396209717\n",
      "loss: 2.4364242553710938\n",
      "loss: 2.3763577938079834\n",
      "loss: 2.399959087371826\n",
      "loss: 2.385770082473755\n",
      "loss: 2.435868263244629\n",
      "loss: 2.3757853507995605\n",
      "loss: 2.3993825912475586\n",
      "loss: 2.3852224349975586\n",
      "loss: 2.435340404510498\n",
      "loss: 2.375246047973633\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for i in range(50):\n",
    "    # forward pass\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(xs)),ys].log().mean()\n",
    "\n",
    "    print(f'loss: {loss}')\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    W.data += -50*W.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aec09fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce.\n",
      "be.\n",
      "memaluriale.\n",
      "kayhamelilisttain.\n",
      "lenak.\n"
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for i in range(5):\n",
    "    idx1, idx2 = 0,0\n",
    "    out = ''\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([[idx1,idx2]]), num_classes=27).float()\n",
    "        xenc = xenc.view(1, 54)\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        idx3 = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out+=itos[idx3]\n",
    "        \n",
    "        # shift context\n",
    "        idx2 = idx3\n",
    "        idx1  = idx2\n",
    "\n",
    "        if idx3 == 0 :\n",
    "            break\n",
    "    print(out)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19084763",
   "metadata": {},
   "source": [
    "train, dev, test sets - 80%, 10%, 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d677c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d87185ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033 --> 25626 + 3203 + 3204\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(words)  # remove ordering bias. fair eval and same distribution is the goal\n",
    "n = len(words)\n",
    "train_len = int(n * 0.8)\n",
    "dev_len = int(n * 0.9)\n",
    "train_set = words[:train_len]\n",
    "dev_set = words[train_len:dev_len]\n",
    "test_set = words[dev_len:]\n",
    "print(f'{n} --> {len(train_set)} + {len(dev_set)} + {len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b76cf507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 2]) torch.Size([182625])\n",
      "torch.Size([182625, 54])\n"
     ]
    }
   ],
   "source": [
    "xs_tr, ys_tr = [], []\n",
    "for w in train_set:\n",
    "    chs = ['.','.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:],chs[2:]):\n",
    "        xs_tr.append([stoi[ch1],stoi[ch2]])\n",
    "        ys_tr.append(stoi[ch3])\n",
    "xs_tr = torch.tensor(xs_tr)\n",
    "ys_tr = torch.tensor(ys_tr)\n",
    "\n",
    "print(xs_tr.shape, ys_tr.shape)\n",
    "\n",
    "xs_tr_enc = F.one_hot(xs_tr,num_classes=27).float()\n",
    "xs_tr_enc = xs_tr_enc.view(len(xs_tr), 2*27)\n",
    "print(xs_tr_enc.shape)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54, 27),requires_grad=True, generator=g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4986398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.35335636138916\n"
     ]
    }
   ],
   "source": [
    "# gradient descent \n",
    "\n",
    "for k in range(500):\n",
    "\n",
    "    # forward pass\n",
    "\n",
    "    logits = xs_tr_enc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(xs_tr_enc)), ys_tr].log().mean()\n",
    "    # print(f'loss: {loss}')\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "\n",
    "    W.data += -5 * W.grad\n",
    "\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1d0b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trigram_xy(words):\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "        chs = ['.', '.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            xs.append([stoi[ch1], stoi[ch2]])\n",
    "            ys.append(stoi[ch3])\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    xenc = xenc.view(len(xs), 54)\n",
    "\n",
    "    return xenc, ys\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(xenc, ys, W):\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = - probs[(torch.arange(len(ys))),ys].log().mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7752a550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3530)\n"
     ]
    }
   ],
   "source": [
    "xenc_d, ys_d = build_trigram_xy(dev_set)\n",
    "dev_loss = evaluate(xenc_d, ys_d, W)\n",
    "print(dev_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d191a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3553)\n"
     ]
    }
   ],
   "source": [
    "xenc_t, ys_t = build_trigram_xy(test_set)\n",
    "test_loss = evaluate(xenc_t, ys_t, W)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744feaf4",
   "metadata": {},
   "source": [
    "Smoothing/regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1230a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_reg(xenc, ys, W, lambda_):\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = - probs[(torch.arange(len(ys))),ys].log().mean()\n",
    "    reg = (lambda_ * (W**2)).mean()\n",
    "    return loss, loss+reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee452ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best lambda: 0.0\n",
      "{0.0: 2.341703414916992, 0.0001: 2.3417139053344727, 0.001: 2.3418095111846924, 0.01: 2.3431200981140137, 0.1: 2.3640191555023193, 1.0: 2.4882419109344482, 1.5: 2.532921552658081}\n"
     ]
    }
   ],
   "source": [
    "results={}\n",
    "lambdas = [0.0, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n",
    "for lambda_ in lambdas:\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.randn((54, 27), requires_grad=True, generator = g)\n",
    "\n",
    "    for k in range(1000):\n",
    "        nll, loss = loss_reg(xs_tr_enc, ys_tr, W, lambda_)\n",
    "\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        W.data += -20* W.grad\n",
    "\n",
    "    dev_loss = evaluate(xenc_d, ys_d, W)\n",
    "    results[lambda_] = dev_loss.item()\n",
    "\n",
    "best_lambda = min(results, key=results.get)\n",
    "print(\"Best lambda:\", best_lambda)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8a3693d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  tensor(2.3440)\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((54, 27), requires_grad=True, generator = g)\n",
    "\n",
    "# train again with best lambda value\n",
    "for k in range(1000):\n",
    "    nll, loss = loss_reg(xs_tr_enc, ys_tr, W, best_lambda)\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -20* W.grad\n",
    "\n",
    "# evaluate on test\n",
    "test_loss = evaluate(xenc_t, ys_t, W)\n",
    "print('test loss: ', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c9919",
   "metadata": {},
   "source": [
    "one hot encoding alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a02359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 2]) torch.Size([228146])\n"
     ]
    }
   ],
   "source": [
    "# complete version\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.','.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:],chs[2:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        idx3 = stoi[ch3]\n",
    "        xs.append([idx1,idx2])\n",
    "        ys.append(idx3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(xs.shape, ys.shape)\n",
    "\n",
    "# xenc = F.one_hot(xs, num_classes=27).float()\n",
    "# xenc = xenc.view(len(xs),54)\n",
    "\n",
    "#initialization\n",
    "g = torch.Generator().manual_seed(456)\n",
    "W = torch.randn((54,27), requires_grad=True, generator=g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc9bc2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0],\n",
       "        [ 0,  5],\n",
       "        [ 5, 13],\n",
       "        ...,\n",
       "        [26, 25],\n",
       "        [25, 26],\n",
       "        [26, 24]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bdeecad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "307ca2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5215535163879395\n"
     ]
    }
   ],
   "source": [
    "# gradient descent \n",
    "\n",
    "for k in range(500):\n",
    "\n",
    "    # forward pass\n",
    "\n",
    "    # logits = xs_tr_enc @ W\n",
    "    logits = W[xs].sum(dim=1) # alternatively using sepearte Ws it can be done as W1[xs[:,0]] + W2[xs[:,1]]\n",
    "\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "    # print(f'loss: {loss}')\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "\n",
    "    W.data += -5 * W.grad\n",
    "\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8797d",
   "metadata": {},
   "source": [
    "cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91993e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5060222148895264\n"
     ]
    }
   ],
   "source": [
    "# gradient descent \n",
    "\n",
    "for k in range(500):\n",
    "\n",
    "    # forward pass\n",
    "\n",
    "    # logits = xs_tr_enc @ W\n",
    "    logits = W[xs].sum(dim=1) # alternatively using sepearte Ws it can be done as W1[xs[:,0]] + W2[xs[:,1]]\n",
    "\n",
    "    # counts = logits.exp()\n",
    "    # probs = counts / counts.sum(1, keepdims=True)\n",
    "    # loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "\n",
    "    # last 3 lines can be implemented as cross entropy = softmax + nll\n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "\n",
    "    # print(f'loss: {loss}')\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "\n",
    "    W.data += -5 * W.grad\n",
    "\n",
    "print(loss.item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
