{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757cf9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f38b837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia']\n"
     ]
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda10c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {v:k+1 for k,v in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {v:k for k,v in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583d6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "\n",
    "def build_dataset(block_size, words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = block_size * [0]\n",
    "        for c in w + '.':\n",
    "            ix = stoi[c]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "block_size = 3\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "train_len = int(len(words)*0.8)\n",
    "val_len = int(len(words)*0.9)\n",
    "\n",
    "X_tr, Y_tr = build_dataset(block_size, words[:train_len])\n",
    "X_d, Y_d = build_dataset(block_size, words[train_len:val_len])\n",
    "X_t, Y_t = build_dataset(block_size, words[val_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5512adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7568632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "# layer 1 \n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g) * (5/3) / ((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "\n",
    "# layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "\n",
    "# batchnorm params\n",
    "bngain = torch.randn((1, n_hidden)) *0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) *0.1 \n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a68ac42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120000, 157809,  82137,  69514,  73004,  68734,    286, 123947,  13538,\n",
       "         42674, 165010,  81021,  59151,  46471,  62456,  64636,  24418, 108817,\n",
       "        169833, 145683, 168275, 157689,  36258, 142280,  32537, 149713, 149734,\n",
       "        149517, 165139, 153533,  89661,  20039])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, X_tr.shape[0], (32,), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2070a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "#construct mini batch\n",
    "ix = torch.randint(0,X_tr.shape[0], (batch_size, ),generator=g)\n",
    "Xb, Yb = X_tr[ix], Y_tr[ix]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0549998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4731, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb]   # embedding\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "\n",
    "# linear layer 1\n",
    "hprebn = embcat @ W1 + b1   # hidden layer pre-activation\n",
    "\n",
    "# batchnorm layer\n",
    "bnmeani = hprebn.sum(0, keepdim = True)/n\n",
    "bndiff = hprebn - bnmeani \n",
    "bndiff2 = bndiff ** 2\n",
    "bnvar = bndiff2.sum(0, keepdim = True) / (n-1)\n",
    "bnvar_inv = (bnvar+1e-5) ** -0.5    # calculate the inverse and eps = 1e-5\n",
    "bnraw = bndiff * bnvar_inv  # normalize\n",
    "\n",
    "hpreact =  bngain * bnraw + bnbias  # scale and shift\n",
    "\n",
    "# non linearity\n",
    "h = torch.tanh(hpreact)  \n",
    "\n",
    "# linear layer 2\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "# cross entropy loss\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability \n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim = True)\n",
    "counts_sum_inv = counts_sum ** -1 #if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact..\n",
    "\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = - logprobs[range(n), Yb].mean() # index: plucks out the logprobs of the correct next char\n",
    "\n",
    "\n",
    "# pytorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, \n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0affb153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 27])\n"
     ]
    }
   ],
   "source": [
    "print(logprobs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adb3a374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape,counts_sum_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "826db510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape , logit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67e48389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64, 27]),\n",
       " torch.Size([27]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, h.shape, W2.shape,  b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74231df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bngain.shape, bnraw.shape, bnbias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9da4776d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bndiff.shape , bnvar_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9796ddbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bndiff2.sum(0, keepdim = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c85e5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]), torch.Size([1, 64]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprebn.shape, bnmeani.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5688a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([32, 30]),\n",
       " torch.Size([30, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprebn.shape,  embcat.shape, W1.shape,  b1.shape \n",
    "# hprebn = embcat @ W1 + b1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b07d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 30]), torch.Size([32, 3, 10]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embcat.shape, emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab36b292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 10]), torch.Size([27, 10]), torch.Size([32, 3]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape, C.shape, Xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e485ee",
   "metadata": {},
   "source": [
    "Exercise 1: backprop through the whole thing manually, backpropagating through exactly all of the variables as they are defined in the forward pass above, one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13b39f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------- logprobs.grad or dlogprobs ------------------------------------------------\n",
    "\n",
    "# calculating logprobs.grad manually i.e. dlogprobs\n",
    "# dlogprobs = derivative of loss w.r.t all the elements of logprobs\n",
    "# i.e. taking derivate of: loss = - logprobs[range(n), Yb].mean()\n",
    "# sample eg: loss = -(a+b+c)/3 \n",
    "# so if derivate is taken w.r.t each element in loss it would be dloss/da = -1/3 i.e. generalized to -1/n\n",
    "\n",
    "#so the derivative for each element indexed by Yb is -1/n and is stored at that index \n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)  # create an array of zeros in the shape of logprobs\n",
    "dlogprobs[range(n), Yb] = -1.0 / n\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "\n",
    "#'---------------------------- probs.grad or dprobs ------------------------------------------------------'\n",
    "# logprobs = probs.log()\n",
    "# loss = - logprobs[range(n), Yb].mean() \n",
    "\n",
    "#chain rule: dloss/dprobs = dloss/dlogprobs * dlogprobs/dprobs where dloss/dlogprobs is already calc above as\n",
    "# dlogprobs and dloss/dprobs would be 1/probs as d/dxlog(x)=1/x\n",
    "dprobs = dlogprobs * (1.0/probs)\n",
    "cmp('probs', dprobs, probs)\n",
    "\n",
    "#'---------------------------- counts_sum_inv.grad or dcounts_sum_inv -----------------------------------'\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = - logprobs[range(n), Yb].mean() \n",
    "\n",
    "# chain rule : dloss/dcounts_sum_inv =  dloss/dprobs * dprobs/dcounts_sum_inv\n",
    "# counts.shape and counts_sum_inv.shape are different so in the multiplication here:probs = counts * counts_sum_inv\n",
    "# there is an implicit broadcasting that pytorch will do so backprop needs to consider that and there would\n",
    "# be a sum so dprobs/dcounts_sum_inv would be \n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs  # dloss/dcounts = dloss/dprobs * dprobs/dcounts\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "#'---------------------------- counts_sum.grad or dcounts_sum -----------------------------------'\n",
    "# counts_sum_inv = counts_sum ** -1 \n",
    "\n",
    "# chain rule : dloss/dcounts_sum =  dloss/dcounts_sum_inv * dcounts_sum_inv/dcounts_sum\n",
    "dcounts_sum = dcounts_sum_inv * (- counts_sum ** -2) \n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "\n",
    "#'---------------------------- counts.grad or dcounts -----------------------------------'\n",
    "# counts_sum = counts.sum(1, keepdim = True)\n",
    "# counts_sum_inv = counts_sum ** -1 \n",
    "\n",
    "# chain rule : dloss/dcounts =  dloss/dcounts_sum * dcounts_sum/dcounts and add to the prev value calc above\n",
    "dcounts += dcounts_sum * torch.ones((n,1))  # or torch.ones_like(counts)\n",
    "cmp('counts', dcounts, counts)\n",
    "\n",
    "#'---------------------------- norm_logits.grad or dnorm_logits -----------------------------------'\n",
    "# counts = norm_logits.exp() \n",
    "\n",
    "# chain rule : dloss/dnorm_logits =  dloss/dcounts * dcounts/dnorm_logits\n",
    "dnorm_logits = dcounts * norm_logits.exp()  # or dcounts * counts as counts = norm_logits.exp()\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "\n",
    "#'---------------------------- logit_maxes.grad or dlogit_maxes -----------------------------------'\n",
    "# norm_logits = logits - logit_maxes  # check shape to know if there is an implicit broadcasting\n",
    "# counts = norm_logits.exp()\n",
    "\n",
    "# chain rule : \n",
    "# dloss/dlogit_maxes =  dloss/dnorm_logits * dnorm_logits/dlogit_maxes\n",
    "# dloss/dlogits =  dloss/dnorm_logits * dnorm_logits/dlogits\n",
    "\n",
    "dlogit_maxes = (dnorm_logits * (-1)).sum(1, keepdim=True)\n",
    "dlogits = dnorm_logits.clone()    # dnorm_logits * 1\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "\n",
    "#'---------------------------- logits.grad or dlogits -----------------------------------'\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes\n",
    "\n",
    "# chain rule : dloss/dlogits =  dloss/dlogit_maxes * dlogit_maxes/dlogits\n",
    "dlogits += dlogit_maxes * F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) # use one hot or\n",
    "# create using torch.zeros and put 1 as the derivative would be 1 for the max number at the indices returned by max func\n",
    "cmp('logits', dlogits, logits)\n",
    "\n",
    "#'---------------------------- logit_maxes.grad or dlogit_maxes -----------------------------------'\n",
    "# logits = h @ W2 + b2\n",
    "\n",
    "# chain rule : \n",
    "# dloss/dh =  dloss/dlogits * dlogits/dh\n",
    "# dloss/dW2 =  dloss/dlogits * dlogits/dW2\n",
    "# dloss/db2 =  dloss/dlogits * dlogits/db2\n",
    "\n",
    "dh = dlogits @ W2.T \n",
    "dW2 = h.T @ dlogits\n",
    "db2 = (dlogits * 1).sum(0)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "\n",
    "#'---------------------------- hpreact.grad or dhpreact -----------------------------------'\n",
    "# h = torch.tanh(hpreact) \n",
    "# chain rule : dloss/dhpreact = dloss/dh * dh/dhpreact\n",
    "\n",
    "dhpreact = dh * (1.0-h**2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "\n",
    "#'---------------------------- dbnbias, dbnraw, dbngain -----------------------------------'\n",
    "# hpreact =  bngain * bnraw + bnbias  # scale and shift\n",
    "# chain rule : \n",
    "# dloss/dbngain = dloss/dhpreact * dhpreact/dbngain\n",
    "# dloss/dbnraw = dloss/dhpreact * dhpreact/dbnraw\n",
    "# dloss/dbnbias = dloss/dhpreact * dhpreact/dbnbias\n",
    "\n",
    "dbngain = (dhpreact * bnraw).sum(0, keepdim=True)  # because of broadcasting\n",
    "dbnraw =  bngain * dhpreact\n",
    "dbnbias = (dhpreact).sum(0, keepdim=True) # because of broadcasting\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "\n",
    "# ----------------------------- dbndiff, dbnvar_inv -----------------------------------\n",
    "# bnraw = bndiff * bnvar_inv  \n",
    "\n",
    "# chain rule : \n",
    "# dloss/dbndiff = dloss/dbnraw * dbnraw/dbndiff\n",
    "# dloss/bnvar_inv = dloss/dbnraw * dbnraw/dbnvar_inv\n",
    "\n",
    "dbndiff = bnvar_inv * dbnraw   \n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim = True) # because of broadcasting\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "# ---------------------------- dbnvar ------------------------------------\n",
    "# bnvar_inv = (bnvar+1e-5) ** -0.5\n",
    "# chain rule : dloss/dbnvar =  dloss/dbnvar_inv * dbnvar_inv/dbnvar\n",
    "dbnvar = (-0.5 * (bnvar+1e-5) ** -1.5) * dbnvar_inv\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "\n",
    "# ---------------------------- dbndiff2 ------------------------------------\n",
    "# bnvar = bndiff2.sum(0, keepdim = True) / (n-1)\n",
    "# chain rule : dloss/dbndiff2 = dloss/dbnvar * dbnvar/dbndiff2\n",
    "# dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar # why is this not working?\n",
    "dbndiff2 = dbnvar / (n-1)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "# ---------------------------- bndiff ------------------------------------\n",
    "# bndiff2 = bndiff ** 2\n",
    "# chain rule : dloss/dbndiff = dloss/dbndiff2 * dbndiff2/dbndiff\n",
    "dbndiff += dbndiff2 *  (2 * bndiff)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "\n",
    "# ---------------------------- dhprebn, dbnmeani ------------------------------------\n",
    "# bndiff = hprebn - bnmeani \n",
    "# chain rule : \n",
    "# dloss/dhprebn = dloss/dbndiff * dbndiff/dhprebn\n",
    "# dloss/dbnmeani = dloss/dbndiff * dbndiff/dbnmeani\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (- dbndiff).sum(0)\n",
    "\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "# ---------------------------- dhprebn ------------------------------------\n",
    "# bnmeani = hprebn.sum(0, keepdim = True)/n\n",
    "# chain rule : dloss/dhprebn = dloss/dbnmeani * dbnmeani/dhprebn\n",
    "\n",
    "dhprebn +=  dbnmeani / n  #(will broadcast)   # or 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "\n",
    "# ---------------------------- dembcat, dW1, db1 ------------------------------------\n",
    "# hprebn = embcat @ W1 + b1 \n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "\n",
    "# ---------------------------- demb, dC  ------------------------------------------------\n",
    "# emb = C[Xb]   # embedding\n",
    "# embcat = emb.view(emb.shape[0], -1)\n",
    "demb = dembcat.view(emb.shape)  # back to orginal shape, just rerepresent the derivatives into original view\n",
    "cmp('emb', demb, emb)\n",
    "\n",
    "# C[Xb] is plucking from the embedding table C for values of Xb\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50bd79b",
   "metadata": {},
   "source": [
    "Exercise 2: backprop through cross_entropy but all in one go to complete this challenge look at the mathematical expression of the loss, take the derivative, simplify the expression, and just write it out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a529509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4731383323669434 diff: -2.384185791015625e-07\n",
      "logits          | exact: False | approximate: True  | maxdiff: 7.2177499532699585e-09\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n",
    "\n",
    "\n",
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "#dloss / dlogits = -1 + logits.exp()/logits.exp().sum() if  i=y else logits.exp()/logits().exp().sum() i.e.softmax\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n),Yb] -= 1\n",
    "dlogits /= n\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d710a59",
   "metadata": {},
   "source": [
    "Exercise 3: backprop through batchnorm but all in one go to complete this challenge look at the mathematical expression of the output of batchnorm, take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "BatchNorm paper: https://arxiv.org/abs/1502.03167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0405b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())\n",
    "\n",
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca1bc1c",
   "metadata": {},
   "source": [
    "Exercise 4: putting it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f411c336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7484\n",
      "  10000/ 200000: 2.2119\n",
      "  20000/ 200000: 2.4039\n",
      "  30000/ 200000: 2.5108\n",
      "  40000/ 200000: 1.9725\n",
      "  50000/ 200000: 2.3981\n",
      "  60000/ 200000: 2.3697\n",
      "  70000/ 200000: 2.0765\n",
      "  80000/ 200000: 2.3480\n",
      "  90000/ 200000: 2.1734\n",
      " 100000/ 200000: 1.9552\n",
      " 110000/ 200000: 2.2345\n",
      " 120000/ 200000: 1.9548\n",
      " 130000/ 200000: 2.4439\n",
      " 140000/ 200000: 2.2514\n",
      " 150000/ 200000: 2.0761\n",
      " 160000/ 200000: 1.9728\n",
      " 170000/ 200000: 1.8697\n",
      " 180000/ 200000: 2.0100\n",
      " 190000/ 200000: 1.9482\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X_tr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = X_tr[ix], Y_tr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! \n",
    "    # -----------------\n",
    "    # cross entropy\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n),Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = (dlogits * 1).sum(0)\n",
    "    # tanh\n",
    "    dhpreact = dh * (1.0-h**2)\n",
    "    # batchnorm \n",
    "    dbngain = (dhpreact * bnraw).sum(0, keepdim=True)  \n",
    "    dbnbias = (dhpreact).sum(0, keepdim=True) \n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)  \n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "        for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k,j]\n",
    "            dC[ix] += demb[k,j]\n",
    "    \n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      # p.data += -lr * p.grad # old way (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way (manual)\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    #   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df684c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.862645149230957e-08\n",
      "(30, 200)       | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
      "(200,)          | exact: False | approximate: True  | maxdiff: 4.190951585769653e-09\n",
      "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
      "(27,)           | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "(1, 200)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate batch norm parameters at the end of training\n",
    "with torch.no_grad():\n",
    "    emb = C[X_tr]\n",
    "    embcat = emb.view(emb.shape[0],-1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure mean, std over entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, unbiased = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eab437d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0708296298980713\n",
      "val 2.106884002685547\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x, y = {'train': (X_tr, Y_tr),\n",
    "            'val' : (X_d, Y_d),\n",
    "            'test' : (X_t, Y_t)}[split]\n",
    "    \n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0],-1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5) ** -0.5 + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03be2a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dex.\n",
      "mariah.\n",
      "makilah.\n",
      "tyha.\n",
      "malissana.\n",
      "nella.\n",
      "kaman.\n",
      "arreliyah.\n",
      "jaxson.\n",
      "mari.\n",
      "moriella.\n",
      "kinzie.\n",
      "darette.\n",
      "kamside.\n",
      "eniavion.\n",
      "rosbut.\n",
      "huniven.\n",
      "tahlyn.\n",
      "kashru.\n",
      "anesley.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    out = ''\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        # --------------\n",
    "        # forward pass\n",
    "        # embedding\n",
    "        emb = C[torch.tensor([context])]\n",
    "        embcat = emb.view(emb.shape[0],-1)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar+1e-5)**-0.5 + bnbias\n",
    "        h = torch.tanh(hpreact)\n",
    "        logits = h @ W2 + b2\n",
    "        #------------------\n",
    "        # sample\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out += itos[ix]\n",
    "\n",
    "        if ix==0:\n",
    "            break\n",
    "    print(out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
